{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import ContextualEnvironment\n",
    "from policies import KLUCBSegmentPolicy, RandomPolicy, ExploreThenCommitSegmentPolicy, EpsilonGreedySegmentPolicy, TSSegmentPolicy, LinearTSPolicy\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_path = \"data/user_features.csv\"\n",
    "users_df = pd.read_csv(users_path)\n",
    "\n",
    "playlists_path = \"data/playlist_features.csv\"\n",
    "playlists_df = pd.read_csv(playlists_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = len(users_df)\n",
    "n_playlists = len(playlists_df)\n",
    "n_recos = 12\n",
    "print_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = np.array(users_df.drop([\"segment\"], axis = 1))\n",
    "user_features = np.concatenate([user_features, np.ones((n_users,1))], axis = 1)\n",
    "playlist_features = np.array(playlists_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_segment = np.array(users_df.segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_env = ContextualEnvironment(user_features, playlist_features, user_segment, n_recos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies_name = \"random\".split(\",\")\n",
    "policies = set_policies(policies_name, user_segment, user_features, n_playlists)\n",
    "n_policies = len(policies)\n",
    "\n",
    "n_users_per_round = 20000\n",
    "n_rounds = 100\n",
    "overall_rewards = np.zeros((n_policies, n_rounds))\n",
    "overall_optimal_reward = np.zeros(n_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'reproducibility_exp/results_%.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1):\n",
    "    print(\"Run: %d\"%(k))\n",
    "    cont_env = ContextualEnvironment(user_features, playlist_features, user_segment, n_recos)    \n",
    "    #print(\"STARTING SIMULATIONS\")\n",
    "    #print(\"for %d rounds, with %d users per round (randomly drawn with replacement)\\n \\n\" % (n_rounds, n_users_per_round))\n",
    "    start_time = time.time()\n",
    "    for i in range(n_rounds):\n",
    "        # Select batch of n_users_per_round users\n",
    "        user_ids = np.random.choice(range(n_users), n_users_per_round)\n",
    "        overall_optimal_reward[i] = np.take(cont_env.th_rewards, user_ids).sum()\n",
    "        # Iterate over all policies\n",
    "        for j in range(n_policies):\n",
    "            # Compute n_recos recommendations\n",
    "            recos = policies[j].recommend_to_users_batch(user_ids, args.n_recos, args.l_init)\n",
    "            # Compute rewards\n",
    "            rewards = cont_env.simulate_batch_users_reward(batch_user_ids= user_ids, batch_recos=recos)\n",
    "            # Update policy based on rewards\n",
    "            policies[j].update_policy(user_ids, recos, rewards, args.l_init)\n",
    "            overall_rewards[j,i] = rewards.sum()\n",
    "        # Print info\n",
    "        #if i == 0 or (i+1) % print_every == 0 or i+1 == n_rounds:\n",
    "        #    print(\"Round: %d/%d. Elapsed time: %f sec.\" % (i+1, n_rounds, time.time() - start_time))\n",
    "        #    print(\"Cumulative regrets: \\n%s \\n\" % \"\\n\".join([\"\t%s : %s\" % (policies_name[j], str(np.sum(overall_optimal_reward - overall_rewards[j]))) for j in range(n_policies)]))\n",
    "\n",
    "    output_path = 'reproducibility_exp/New_enviro_results_%d.json'%(k)\n",
    "    print(\"Saving cumulative regrets in %s\" % output_path)\n",
    "    cumulative_regrets = {policies_name[j] : list(np.cumsum(overall_optimal_reward - overall_rewards[j])) for j in range(n_policies)}\n",
    "    with open(output_path, 'w') as fp:\n",
    "        json.dump(cumulative_regrets, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e33c0869bf9e879f87da6a894846076cd9e2102e1efd959a79c5d33a31a98db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
